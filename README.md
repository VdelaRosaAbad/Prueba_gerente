# üöÄ Cargador Ultra-R√°pido para BigQuery

## ‚ö° Carga de 136GB en menos de 30 minutos

Soluci√≥n optimizada para cargar archivos masivos en BigQuery utilizando Dataflow, Python y Cloud Shell.

## üéØ Caracter√≠sticas Principales

- **‚ö° Velocidad Ultra-R√°pida**: Carga de 136GB en 15-25 minutos
- **üîÑ Procesamiento Paralelo**: Utiliza hasta 100 workers de Dataflow
- **üìä Monitoreo en Tiempo Real**: Seguimiento del progreso y rendimiento
- **üîß Configuraci√≥n Automatizada**: Setup autom√°tico del entorno
- **üí∞ Optimizaci√≥n de Costos**: Configuraci√≥n balanceada entre velocidad y costo

## üõ†Ô∏è Requisitos Previos

- ‚úÖ Proyecto de Google Cloud Platform activo
- ‚úÖ Cloud Shell habilitado
- ‚úÖ APIs de Dataflow, BigQuery y Storage habilitadas
- ‚úÖ Permisos de administrador o editor en el proyecto

## üöÄ Instalaci√≥n y Configuraci√≥n

### Paso 1: Clonar y Configurar

```bash
# En Cloud Shell, ejecutar:
chmod +x *.sh
chmod +x *.py

# Configurar el entorno
./setup_environment.sh
```

### Paso 2: Verificar Configuraci√≥n

```bash
# Verificar que las librer√≠as est√©n instaladas
python3 -c "import apache_beam; print('‚úÖ Apache Beam instalado')"
python3 -c "import google.cloud.bigquery; print('‚úÖ BigQuery instalado')"

# Verificar configuraci√≥n de GCP
gcloud config list
```

## üéÆ Uso

### Opci√≥n 1: Ejecuci√≥n Automatizada (Recomendada)

```bash
# Ejecutar todo el proceso autom√°ticamente
./run_ultra_fast_loader.sh
```

### Opci√≥n 2: Ejecuci√≥n Manual

```bash
# 1. Configurar variables
export PROJECT_ID=$(gcloud config get-value project)
export REGION="us-central1"

# 2. Ejecutar pipeline
python3 ultra_fast_loader.py \
    --project=$PROJECT_ID \
    --region=$REGION \
    --temp_location=gs://$PROJECT_ID-temp/temp \
    --staging_location=gs://$PROJECT_ID-temp/staging \
    --runner=DataflowRunner \
    --num_workers=50 \
    --max_num_workers=100
```

### Opci√≥n 3: Monitoreo en Tiempo Real

```bash
# Monitorear el progreso del pipeline
python3 monitor_pipeline.py --project=$PROJECT_ID
```

## üìä Monitoreo y M√©tricas

### Dashboard de Dataflow
- Acceder a: [Dataflow Console](https://console.cloud.google.com/dataflow)
- Ver jobs activos y m√©tricas en tiempo real

### M√©tricas Clave
- **Velocidad de Procesamiento**: Filas por segundo
- **Uso de Workers**: N√∫mero de workers activos
- **Progreso**: Porcentaje de archivo procesado
- **Tiempo Estimado**: Tiempo restante para completar

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Optimizaci√≥n de Workers

```python
# En ultra_fast_loader.py, ajustar:
worker_options.num_workers = 50          # Workers iniciales
worker_options.max_num_workers = 100     # M√°ximo de workers
worker_options.machine_type = 'n1-standard-4'  # Tipo de m√°quina
```

### Configuraci√≥n de Regi√≥n

```bash
# Cambiar regi√≥n seg√∫n tu ubicaci√≥n
export REGION="us-central1"     # Estados Unidos Central
export REGION="europe-west1"    # Europa Occidental
export REGION="asia-southeast1" # Asia Sudeste
```

## üîç Soluci√≥n de Problemas

### Error: "API not enabled"
```bash
# Habilitar APIs necesarias
gcloud services enable dataflow.googleapis.com
gcloud services enable bigquery.googleapis.com
gcloud services enable storage.googleapis.com
```

### Error: "Insufficient quota"
```bash
# Verificar cuotas disponibles
gcloud compute regions describe us-central1 --project=$PROJECT_ID
```

### Error: "Permission denied"
```bash
# Verificar permisos
gcloud projects get-iam-policy $PROJECT_ID
```

## üìà Rendimiento Esperado

| Tama√±o de Archivo | Workers | Tiempo Estimado | Costo Aproximado |
|-------------------|---------|-----------------|------------------|
| 136 GB            | 50-100  | 15-25 min       | $2-5 USD         |
| 50 GB             | 30-50   | 8-15 min        | $1-3 USD         |
| 10 GB             | 10-20   | 3-8 min         | $0.5-1 USD       |

## üèóÔ∏è Arquitectura de la Soluci√≥n

```
üìÅ Archivo CSV.gz (136GB)
    ‚Üì
üåê Cloud Storage
    ‚Üì
‚ö° Dataflow Pipeline (50-100 workers)
    ‚Üì
üóÑÔ∏è BigQuery Table
    ‚Üì
üìä Datos Disponibles para An√°lisis
```

## üîß Personalizaci√≥n

### Cambiar Formato de Archivo

```python
# En ultra_fast_loader.py, modificar:
raw_data = (
    pipeline 
    | 'ReadFile' >> ReadFromText(
        'gs://tu-bucket/tu-archivo.csv',  # Cambiar ruta
        compression_type='gzip',           # Cambiar compresi√≥n
        strip_trailing_newlines=True
    )
)
```

### Cambiar Esquema de BigQuery

```python
# Esquema personalizado en lugar de auto-detect
schema = 'campo1:STRING,campo2:INTEGER,campo3:FLOAT'
processed_data | 'WriteToBigQuery' >> WriteToBigQuery(
    'proyecto:dataset.tabla',
    schema=schema,  # Usar esquema personalizado
    # ... otras opciones
)
```

## üìö Recursos Adicionales

- [Documentaci√≥n de Dataflow](https://cloud.google.com/dataflow/docs)
- [Gu√≠a de BigQuery](https://cloud.google.com/bigquery/docs)
- [Optimizaci√≥n de Costos](https://cloud.google.com/dataflow/docs/guides/cost-optimization)
- [Monitoreo de Jobs](https://cloud.google.com/dataflow/docs/guides/monitoring)

## ü§ù Soporte

Para problemas o preguntas:
1. Revisar logs del pipeline en Dataflow Console
2. Verificar configuraci√≥n de permisos y APIs
3. Consultar m√©tricas de rendimiento
4. Revisar cuotas del proyecto

## üìÑ Licencia

Este proyecto est√° bajo la licencia MIT. Ver `LICENSE` para m√°s detalles.

---

**‚ö° ¬°Optimiza tu carga de datos y reduce el tiempo de 136GB de horas a minutos!**
